{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a11d13",
   "metadata": {},
   "source": [
    "# catnip\n",
    "## Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40d49a",
   "metadata": {},
   "source": [
    "### Colab only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# clone repo\n",
    "if not os.path.exists(\"/content/catnip\"):\n",
    "    !git clone -b yolo-bbo --recurse-submodules https://github.com/rifusaki/catnip.git\n",
    "    %cd /content/catnip\n",
    "else:\n",
    "    %cd /content/catnip\n",
    "    !git pull\n",
    "\n",
    "# google auth\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# gcsfuse\n",
    "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
    "!apt -qq update\n",
    "!apt -qq install gcsfuse\n",
    "\n",
    "# mount bucket\n",
    "!mkdir -p /content/gcs\n",
    "!gcsfuse --implicit-dirs catnip-data /content/gcs\n",
    "\n",
    "# link bucket to expected data path in pipeline.yaml\n",
    "# pipeline.yaml expects 'catnip-data/' in the repo root\n",
    "!ln -s /content/gcs /content/catnip/catnip-data\n",
    "\n",
    "# install packages if needed\n",
    "%pip install ultralytics pydantic pydantic-settings omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ea118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path configuration\n",
    "from pathlib import Path\n",
    "\n",
    "# define repo root\n",
    "repo_root = Path(\"/content/catnip\").resolve()\n",
    "\n",
    "# add to sys.path\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.append(str(repo_root))\n",
    "\n",
    "# change working directory\n",
    "os.chdir(repo_root)\n",
    "print(f\"Working directory set to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6994cc05",
   "metadata": {},
   "source": [
    "### Local only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880f16d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: C:\\Users\\rifusaki\\Desktop\\catnip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# after cloning rifusaki/catnip\n",
    "# add repository root to path\n",
    "repo_root = Path(\"..\").resolve()\n",
    "\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.append(str(repo_root))\n",
    "\n",
    "# change working directory to repo root\n",
    "os.chdir(repo_root)\n",
    "print(f\"Working directory set to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d055c0b3",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6efc10",
   "metadata": {},
   "source": [
    "### Paths and run config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43bea325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Root: catnip-data\n",
      "Manga Dir: catnip-data\\data\\manga\n",
      "Annotations Dir: catnip-data\\data\\annotations\n",
      "Models Dir: catnip-data\\models\n",
      "Runs Dir: catnip-data\\runs\n",
      "Output Dir: catnip-data\\results\n",
      "Label Studio export Dir: catnip-data\\data\\ls-exports\n"
     ]
    }
   ],
   "source": [
    "# load configuration\n",
    "from src.config import load_settings\n",
    "\n",
    "settings = load_settings()\n",
    "\n",
    "print(f\"Data Root: {settings.paths.data}\")\n",
    "print(f\"Manga Dir: {settings.paths.manga_dir}\")\n",
    "print(f\"Annotations Dir: {settings.paths.annotations_dir}\")\n",
    "print(f\"Models Dir: {settings.paths.model_dir}\")\n",
    "print(f\"Runs Dir: {settings.paths.runs_dir}\")\n",
    "print(f\"Output Dir: {settings.paths.output_dir}\")\n",
    "print(f\"Label Studio export Dir: {settings.paths.ls_exports_dir}\")\n",
    "\n",
    "run_name = \"0.11.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb441daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created symlink: C:\\Users\\rifusaki\\Desktop\\catnip\\yolo_dataset\\images -> C:\\Users\\rifusaki\\Desktop\\catnip\\catnip-data\\data\\manga\n",
      "Created symlink: C:\\Users\\rifusaki\\Desktop\\catnip\\yolo_dataset\\labels -> C:\\Users\\rifusaki\\Desktop\\catnip\\catnip-data\\data\\annotations\n"
     ]
    }
   ],
   "source": [
    "# prepare dataset for YOLO\n",
    "\n",
    "# we create a local 'yolo_dataset' folder with symlinks to the actual data\n",
    "# this ensures a standard structure (images/labels) regardless of the source layout\n",
    "# in my case I have manga/annotations instead of images/labels\n",
    "from src.training.preparation import safe_symlink\n",
    "\n",
    "dataset_root = repo_root / \"yolo_dataset\"\n",
    "dataset_root.mkdir(exist_ok=True)\n",
    "\n",
    "images_link = dataset_root / \"images\"\n",
    "labels_link = dataset_root / \"labels\"\n",
    "\n",
    "# symlink the actual data to this local structure\n",
    "safe_symlink(settings.paths.manga_dir, images_link)\n",
    "safe_symlink(settings.paths.annotations_dir, labels_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e35f5b",
   "metadata": {},
   "source": [
    "### Training prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c03c13b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Label Studio export: catnip-data\\data\\ls-exports\\251228_v1.json\n",
      "Processed 76 tasks. Labels saved to catnip-data\\data\\annotations\n"
     ]
    }
   ],
   "source": [
    "# for some reason the YOLO export from Label Studio doesn't include the file paths\n",
    "# and since my images are not in a flat structure, I cannot use it directly\n",
    "# so, I export the JSON which includes the paths and convert it to YOLO format here\n",
    "from src.convert_labels import convert_label_studio_to_yolo\n",
    "\n",
    "# define path to Label Studio export (adjust filename as needed)\n",
    "ls_export_dir = settings.paths.ls_exports_dir\n",
    "json_files = list(ls_export_dir.glob(\"*.json\"))\n",
    "\n",
    "if json_files:\n",
    "    # pick the latest one or specific one\n",
    "    json_file = sorted(json_files)[-1] \n",
    "    print(f\"Found Label Studio export: {json_file}\")\n",
    "    \n",
    "    convert_label_studio_to_yolo(\n",
    "        json_file, \n",
    "        settings.paths.annotations_dir, \n",
    "        { \"izutsumi\": 0, \"izutsumi_face\": 1, \"thistle\": 2, \"kabru\": 3 }\n",
    "    )\n",
    "else:\n",
    "    print(f\"No Label Studio JSON export found in {ls_export_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a2bbfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating new training list: catnip-data\\yoloConfig\\251228_v1_train.txt\n",
      "found 2752 total images in 'images' directory.\n",
      "generated catnip-data\\yoloConfig\\251228_v1_train.txt\n",
      "   - labeled images (subset): 92\n",
      "   - unlabeled images (skipped): 2660\n"
     ]
    }
   ],
   "source": [
    "# Generate Training List\n",
    "from src.training.preparation import generate_training_list\n",
    "\n",
    "# Define config directory for YOLO artifacts\n",
    "config_dir = settings.paths.data / \"yoloConfig\"\n",
    "config_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Determine filename\n",
    "if 'json_file' in locals() and json_file.exists():\n",
    "    train_list_name = f\"{json_file.stem}_train.txt\"\n",
    "else:\n",
    "    raise RuntimeError(\"No Label Studio JSON export found; cannot determine training list name.\")\n",
    "    # train_list_name = \"train.txt\"  # --- IGNORE ---\n",
    "\n",
    "train_list_path = config_dir / train_list_name\n",
    "\n",
    "# generate list using the symlinked 'images' directory\n",
    "# this ensures paths in the txt file match what YOLO sees in 'yolo_dataset'\n",
    "# note: We use the *symlinked* path for generation so the txt file contains local paths\n",
    "train_list_path = generate_training_list(\n",
    "    images_link, \n",
    "    labels_link, \n",
    "    train_list_path, \n",
    "    force_regenerate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51ad8f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataset.yaml\n",
      "names:\n",
      "  0: izutsumi\n",
      "  1: izutsumi_face\n",
      "  2: thistle\n",
      "  3: kabru\n",
      "path: C:\\Users\\rifusaki\\Desktop\\catnip\\yolo_dataset\n",
      "train: C:\\Users\\rifusaki\\Desktop\\catnip\\catnip-data\\yoloConfig\\251228_v1_train.txt\n",
      "val: C:\\Users\\rifusaki\\Desktop\\catnip\\catnip-data\\yoloConfig\\251228_v1_train.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataset.yaml\n",
    "import importlib\n",
    "import src.training.preparation\n",
    "importlib.reload(src.training.preparation)\n",
    "from src.training.preparation import create_dataset_yaml\n",
    "\n",
    "dataset_yaml_path = create_dataset_yaml(\n",
    "    path=dataset_root, # Point to the local dataset root containing images/ and labels/\n",
    "    train_path=train_list_path,\n",
    "    val_path=train_list_path,\n",
    "    names={0: 'izutsumi', 1: 'izutsumi_face', 2: 'thistle', 3: 'kabru'}\n",
    ")\n",
    "\n",
    "with open(dataset_yaml_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167d6cdb",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1536ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.241  Python-3.13.11 torch-2.6.0 CPU (Intel Core i5-8250U 1.60GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=0.11.0, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=catnip-data\\runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\rifusaki\\Desktop\\catnip\\catnip-data\\runs\\0.11.0, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
      " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
      " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 23        [16, 19, 22]  1    820956  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]          \n",
      "YOLO11s summary: 181 layers, 9,429,340 parameters, 9,429,324 gradients, 21.6 GFLOPs\n",
      "\n",
      "Transferred 493/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 413.6124.3 MB/s, size: 638.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\rifusaki\\Desktop\\catnip\\yolo_dataset\\labels\\v01... 92 images, 16 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 92/92 199.0it/s 0.5s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\rifusaki\\Desktop\\catnip\\yolo_dataset\\labels\\v01.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 515.9147.4 MB/s, size: 819.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\rifusaki\\Desktop\\catnip\\yolo_dataset\\labels\\v01.cache... 92 images, 16 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 92/92 91.9Kit/s 0.0s\n",
      "Plotting labels to C:\\Users\\rifusaki\\Desktop\\catnip\\catnip-data\\runs\\0.11.0\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\rifusaki\\Desktop\\catnip\\catnip-data\\runs\\0.11.0\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/100         0G      1.499      3.989      1.434        143        640: 100% ━━━━━━━━━━━━ 6/6 37.4s/it 3:44.5s:60\n",
      "WARNING NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 33% ━━━━──────── 1/3 99.5s/it 29.9s<3:19WARNING NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 67% ━━━━━━━━──── 2/3 45.9s/it 50.2s<45.9sWARNING NMS time limit 3.400s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 22.5s/it 1:07\n",
      "                   all         92        386     0.0482     0.0892      0.021     0.0111\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      2/100         0G      1.257      2.817      1.249         59        640: 100% ━━━━━━━━━━━━ 6/6 32.7s/it 3:1636.0s\n",
      "WARNING NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 33% ━━━━──────── 1/3 90.2s/it 27.1s<3:00WARNING NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 67% ━━━━━━━━──── 2/3 47.0s/it 49.2s<47.0sWARNING NMS time limit 3.400s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 22.4s/it 1:07\n",
      "                   all         92        386      0.314     0.0993     0.0393     0.0242\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      3/100         0G      1.179      1.998      1.164         81        640: 100% ━━━━━━━━━━━━ 6/6 32.5s/it 3:1536.1s\n",
      "WARNING NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 33% ━━━━──────── 1/3 91.1s/it 27.3s<3:02WARNING NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 67% ━━━━━━━━──── 2/3 47.0s/it 49.4s<47.0sWARNING NMS time limit 3.400s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 22.1s/it 1:06\n",
      "                   all         92        386      0.304      0.268      0.204      0.134\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      4/100         0G      1.115      1.743      1.141         85        640: 83% ━━━━━━━━━━── 5/6 13.2s/it 2:54<13.2s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[32m      3\u001b[39m model = YOLO(\u001b[33m\"\u001b[39m\u001b[33myolo11s.pt\u001b[39m\u001b[33m\"\u001b[39m) \n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_yaml_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m.\u001b[49m\u001b[43mruns_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\ultralytics\\engine\\model.py:773\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    771\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:243\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    240\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:427\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    425\u001b[39m     loss, \u001b[38;5;28mself\u001b[39m.loss_items = unwrap_model(\u001b[38;5;28mself\u001b[39m.model).loss(batch, preds)\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     loss, \u001b[38;5;28mself\u001b[39m.loss_items = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[38;5;28mself\u001b[39m.loss = loss.sum()\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK != -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:136\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Perform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[32m    124\u001b[39m \n\u001b[32m    125\u001b[39m \u001b[33;03mIf x is a dict, calculates and returns the loss for training. Otherwise, returns predictions for inference.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    133\u001b[39m \u001b[33;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predict(x, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:327\u001b[39m, in \u001b[36mBaseModel.loss\u001b[39m\u001b[34m(self, batch, preds)\u001b[39m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m.criterion = \u001b[38;5;28mself\u001b[39m.init_criterion()\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.criterion(preds, batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:137\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:154\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:176\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    177\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:1461\u001b[39m, in \u001b[36mC2PSA.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m   1452\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m   1453\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Process the input tensor through a series of PSA blocks.\u001b[39;00m\n\u001b[32m   1454\u001b[39m \n\u001b[32m   1455\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1459\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor after processing.\u001b[39;00m\n\u001b[32m   1460\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1461\u001b[39m     a, b = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.split((\u001b[38;5;28mself\u001b[39m.c, \u001b[38;5;28mself\u001b[39m.c), dim=\u001b[32m1\u001b[39m)\n\u001b[32m   1462\u001b[39m     b = \u001b[38;5;28mself\u001b[39m.m(b)\n\u001b[32m   1463\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(torch.cat((a, b), \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:78\u001b[39m, in \u001b[36mConv.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     70\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply convolution, batch normalization and activation to input tensor.\u001b[39;00m\n\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:432\u001b[39m, in \u001b[36mSiLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifusaki\\Desktop\\catnip\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\functional.py:2379\u001b[39m, in \u001b[36msilu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   2377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace=inplace)\n\u001b[32m   2378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m-> \u001b[39m\u001b[32m2379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.silu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Try to import Intel Extension for PyTorch\n",
    "try:\n",
    "    import intel_extension_for_pytorch as ipex\n",
    "    print(\"Intel Extension for PyTorch loaded\")\n",
    "except ImportError:\n",
    "    print(\"Intel Extension for PyTorch not found\")\n",
    "\n",
    "# Determine device\n",
    "if hasattr(torch, 'xpu') and torch.xpu.is_available():\n",
    "    device = 'xpu'\n",
    "else:\n",
    "    device = settings.params.device\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = YOLO(\"yolo11s.pt\") \n",
    "\n",
    "results = model.train(\n",
    "    data=str(dataset_yaml_path),\n",
    "    epochs=100,\n",
    "    imgsz=settings.params.img_size,\n",
    "    project=str(settings.paths.runs_dir),\n",
    "    name=run_name,\n",
    "    device=device,\n",
    "    cache=False,\n",
    "    exist_ok=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98542ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "from src.training.preparation import save_best_model\n",
    "\n",
    "save_best_model(\n",
    "    project_dir=settings.paths.runs_dir,\n",
    "    run_name=run_name,\n",
    "    target_dir=settings.paths.model_dir,\n",
    "    target_name=f\"yolo11_izutsumi_{run_name}.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228118fd",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from src.output.output import save_inference_results\n",
    "import torch\n",
    "\n",
    "# Try to import Intel Extension for PyTorch\n",
    "try:\n",
    "    import intel_extension_for_pytorch as ipex\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Determine device\n",
    "if hasattr(torch, 'xpu') and torch.xpu.is_available():\n",
    "    device = 'xpu'\n",
    "else:\n",
    "    device = settings.params.device # Fallback to config\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the trained model\n",
    "model_path = settings.paths.runs_dir / run_name / \"weights\" / \"best.pt\"\n",
    "\n",
    "if not model_path.exists():\n",
    "    print(f\"Model not found at {model_path}\")\n",
    "else:\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    # Define output directories\n",
    "    output_dir = settings.paths.output_dir\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    inference_source = settings.paths.manga_dir\n",
    "    print(f\"Running inference on {inference_source}...\")\n",
    "\n",
    "    # Run prediction\n",
    "    results = model.predict(\n",
    "        source=str(inference_source) + \"/**/*.*\",\n",
    "        project=str(output_dir),\n",
    "        name=run_name,\n",
    "        save=False,\n",
    "        save_txt=False,\n",
    "        conf=0.25,\n",
    "        stream=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    save_inference_results(results, output_dir, inference_source)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
