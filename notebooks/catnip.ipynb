{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d232c2",
   "metadata": {},
   "source": [
    "# Catnip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload for debugging\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56767293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scripts for Colab ---\n",
    "# Clone repo\n",
    "!git clone -b yolo-finetune --recurse-submodules https://github.com/rifusaki/catnip.git\n",
    "%cd /content/catnip\n",
    "!git checkout yolo-finetune\n",
    "\n",
    "# Authenticate with Google\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Install gcsfuse\n",
    "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
    "!apt -qq update\n",
    "!apt -qq install gcsfuse\n",
    "\n",
    "# Mount bucket\n",
    "!mkdir -p /content/catnip/data\n",
    "!gcsfuse --implicit-dirs catnip-data /content/catnip/data\n",
    "\n",
    "# Install packages not included in Colab\n",
    "%pip install ultralytics pydantic pydantic-settings omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d13ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: /Users/rifusaki/repos/catnip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Move working directory to project root (one level up from /notebooks)\n",
    "os.chdir(Path.cwd().parent) # Local\n",
    "os.chdir(str(Path.cwd())+'/catnip') # Colab\n",
    "\n",
    "print(\"Working directory set to:\", Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faec48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies and configuration\n",
    "from src.config import settings, setup_dirs\n",
    "\n",
    "izutsumiPaths, notIzutsumiPaths = setup_dirs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9572788",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe7b7ba",
   "metadata": {},
   "source": [
    "### Panel extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5439a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.coreMPE.src.adenzu_panel.image_processing import panel\n",
    "\n",
    "\n",
    "_ = panel.extract_panels_for_images_in_folder_recursive(\n",
    "    input_dir=str(settings.paths.pages_dir),\n",
    "    output_dir=str(settings.paths.panels_dir),\n",
    "    split_joint_panels=False,   # maps to --split-joint-panels\n",
    "    fallback=True              # maps to --fallback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e7198",
   "metadata": {},
   "source": [
    "### Head crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bbf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess.headExtraction import anime_extraction_recursive\n",
    "\n",
    "\n",
    "valid_exts = {\".jpg\", \".jpeg\", \".png\"}\n",
    "panel_paths = sorted(\n",
    "    [p for p in settings.paths.panels_dir.iterdir() if p.suffix.lower() in valid_exts]\n",
    ")\n",
    "num_crops = anime_extraction_recursive()\n",
    "\n",
    "print(f\"Extracted {num_crops} faces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfcd928",
   "metadata": {},
   "source": [
    "## Catnip core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a871df",
   "metadata": {},
   "source": [
    "### MobileNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.recognition.embeddingModel import compute_embeddings, build_model, load_embeddings\n",
    "\n",
    "\n",
    "# Build fresh model without loading weights\n",
    "embed_model = build_model(settings.params.img_size, settings.paths.crops_dir, load_weights=False)\n",
    "compute_embeddings(embed_model, settings.paths.crops_dir, settings.params.img_size)\n",
    "\n",
    "\n",
    "# Build embedding model (loads saved weights if available)\n",
    "embed_model = build_model(settings.params.img_size, settings.paths.crops_dir, load_weights=True)\n",
    "embs, crop_paths = load_embeddings(settings.paths.embs_dir, settings.paths.crops_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.recognition.query import izutsumi_query, izutsuminess_rank\n",
    "\n",
    "\n",
    "crop, index, score, thre = izutsumi_query(settings.paths.embs_dir,\n",
    "                                        settings.paths.crops_dir,\n",
    "                                        settings.params.img_size, \n",
    "                                        embed_model, \n",
    "                                        izutsumiPaths,\n",
    "                                        notIzutsumiPaths,\n",
    "                                        similarity_threshold=-1,\n",
    "                                        alpha=0.5,\n",
    "                                        mode='max')\n",
    "\n",
    "index_log = izutsuminess_rank(settings.paths.embs_dir, settings.paths.crops_dir, embed_model, izutsumiPaths, notIzutsumiPaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a99d7",
   "metadata": {},
   "source": [
    "### YOLOv8 fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1156e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.preparation import prepare_data\n",
    "\n",
    "prepare_data(izutsumiPaths, notIzutsumiPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562f9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Apple Silicon with MPS enabled on izutsumiTraining.yaml\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09daddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.training import train_model, build_model\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(settings.paths.model_dir/'yolov8_izutsumi_finetuned.pt')\n",
    "\n",
    "model.train(\n",
    "        data=\"config/izutsumiTraining.yaml\",\n",
    "        epochs=50,\n",
    "        imgsz=settings.params.img_size,\n",
    "        batch=16,\n",
    "        lr0=1e-4,       # lower LR for finetuning\n",
    "        freeze=9,      # freeze backbone layers\n",
    "        project=\"runs/izutsumi_finetune\",\n",
    "        name=\"exp1\",\n",
    "        device='cpu',\n",
    "        workers=8,\n",
    "        resume=False,\n",
    "        cache=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e54a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "metrics = model.val()\n",
    "print(metrics)\n",
    "\n",
    "# Predict on unseen images\n",
    "model.predict(\n",
    "    source=\"data/recognition/izutsumiTraining/val/images\",\n",
    "    save=True,\n",
    "    conf=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7568c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"data/models/yolov8_izutsumi_finetuned.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54770429",
   "metadata": {},
   "source": [
    "## Outputting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355d4b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.output.output import save_similar_results, char_nearest_neighbor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b5c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(range(len(index)), score)\n",
    "plt.xlabel(\"Izutsuminess\")\n",
    "plt.ylabel(\"Ranked Results\")\n",
    "plt.title(\"Embed score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c80bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0\n",
    "\n",
    "results = char_nearest_neighbor(crop, index[cutoff:], score[cutoff:], thre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_similar_results(crop, index, settings.paths.output_dir, score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
