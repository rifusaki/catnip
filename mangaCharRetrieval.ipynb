{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character retrieval\n",
    "\n",
    "A starter notebook that extracts panel crops, proposes head candidates with simple heuristics, computes embeddings with TensorFlow (MobileNetV2), and performs nearest-neighbor retrieval using scikit-learn.\n",
    "\n",
    "Place your page images in a folder and update the `PAGES_DIR` variable in the first code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies and configuration\n",
    "PAGES_DIR = 'data/pages'  # change this to where your JPGs are\n",
    "PANELS_DIR = 'data/panels'\n",
    "CROPS_DIR = 'data/crops'\n",
    "import os\n",
    "os.makedirs(PANELS_DIR, exist_ok=True)\n",
    "os.makedirs(CROPS_DIR, exist_ok=True)\n",
    "print('Setup done. Update PAGES_DIR if needed.')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de55214",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Test script to verify OpenCV works\n",
    "import sys\n",
    "print(f\"Python path: {sys.executable}\")\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "    print(f\"OpenCV version: {cv2.__version__}\")\n",
    "    print(\"✅ OpenCV imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ OpenCV import failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ffaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, glob, os, json\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid panel extraction: contours + Hough-based refinement\n",
    "def hybrid_extract_panels(page_path, out_dir, min_area=20000):\n",
    "    img = cv2.imread(page_path)\n",
    "    if img is None:\n",
    "        return 0\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, th = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Contour detection\n",
    "    contours, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    base = Path(page_path).stem\n",
    "    saved = 0\n",
    "\n",
    "    for i, cnt in enumerate(contours):\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area < min_area:\n",
    "            continue\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        panel = img[y:y+h, x:x+w]\n",
    "\n",
    "        # Refine with Hough line detection (split by gutters)\n",
    "        g = cv2.cvtColor(panel, cv2.COLOR_BGR2GRAY)\n",
    "        edges = cv2.Canny(g, 50, 150, apertureSize=3)\n",
    "        lines = cv2.HoughLinesP(edges, rho=1, theta=np.pi/180, threshold=200,\n",
    "                                minLineLength=int(0.5*min(panel.shape[:2])),\n",
    "                                maxLineGap=20)\n",
    "\n",
    "        if lines is not None:\n",
    "            # Split along detected horizontal/vertical lines\n",
    "            masks = []\n",
    "            for line in lines:\n",
    "                x1,y1,x2,y2 = line[0]\n",
    "                if abs(x2-x1) < 20 or abs(y2-y1) < 20:  # vertical or horizontal\n",
    "                    mask = np.zeros_like(g)\n",
    "                    cv2.line(mask, (x1,y1), (x2,y2), 255, thickness=10)\n",
    "                    masks.append(mask)\n",
    "            if masks:\n",
    "                combined = np.zeros_like(g)\n",
    "                for m in masks: combined = cv2.bitwise_or(combined, m)\n",
    "                inv = cv2.bitwise_not(combined)\n",
    "                segs, _ = cv2.findContours(inv, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                for j, s in enumerate(segs):\n",
    "                    xx,yy,ww,hh = cv2.boundingRect(s)\n",
    "                    if ww*hh > min_area/2:\n",
    "                        crop = panel[yy:yy+hh, xx:xx+ww]\n",
    "                        out_path = os.path.join(out_dir, f\"{base}_panel_{i}_{j}.jpg\")\n",
    "                        cv2.imwrite(out_path, crop)\n",
    "                        saved += 1\n",
    "                continue\n",
    "\n",
    "        # If no useful lines, just save the contour box\n",
    "        out_path = os.path.join(out_dir, f\"{base}_panel_{i}.jpg\")\n",
    "        cv2.imwrite(out_path, panel)\n",
    "        saved += 1\n",
    "    return saved\n",
    "\n",
    "# Run panel extraction on all pages\n",
    "page_paths = sorted(glob.glob(os.path.join(PAGES_DIR, '*.jpg')) + glob.glob(os.path.join(PAGES_DIR, '*.png')))\n",
    "count = 0\n",
    "for p in page_paths:\n",
    "    count += hybrid_extract_panels(p, PANELS_DIR)\n",
    "print('Saved', count, 'panels to', PANELS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristic head candidate extraction\n",
    "panel_paths = sorted(glob.glob(os.path.join(PANELS_DIR, '*.jpg')))\n",
    "count = 0\n",
    "for p in panel_paths:\n",
    "    img = cv2.imread(p)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, th = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(th, connectivity=8)\n",
    "    base = Path(p).stem\n",
    "    for i in range(1, num_labels):\n",
    "        x,y,w,h,area = stats[i]\n",
    "        if area < 800:\n",
    "            continue\n",
    "        aspect = w / float(h)\n",
    "        if 0.35 < aspect < 1.8:\n",
    "            pad_w = int(w*0.4); pad_h = int(h*0.6)\n",
    "            x0 = max(0, x-pad_w); y0 = max(0, y-pad_h)\n",
    "            x1 = min(img.shape[1], x+w+pad_w); y1 = min(img.shape[0], y+h+pad_h)\n",
    "            crop = img[y0:y1, x0:x1]\n",
    "            outp = os.path.join(CROPS_DIR, f\"{base}_crop_{i}.jpg\")\n",
    "            cv2.imwrite(outp, crop)\n",
    "            count += 1\n",
    "print('Saved', count, 'candidate crops to', CROPS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f17916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anime face detection with YOLOv8\n",
    "# Load the model\n",
    "model = YOLO(\"yolov8x6_animeface.pt\")\n",
    "\n",
    "# Detect faces in panels and save crops\n",
    "import glob\n",
    "panel_paths = sorted(glob.glob(os.path.join(PANELS_DIR, '*.jpg')))\n",
    "count = 0\n",
    "for p in panel_paths:\n",
    "    results = model.predict(p, imgsz=512, conf=0.3, verbose=False)\n",
    "    base = Path(p).stem\n",
    "    img = cv2.imread(p)\n",
    "    for i, box in enumerate(results[0].boxes.xyxy.cpu().numpy()):\n",
    "        x1, y1, x2, y2 = box.astype(int)\n",
    "        crop = img[y1:y2, x1:x2]\n",
    "        if crop.size == 0: \n",
    "            continue\n",
    "        outp = os.path.join(CROPS_DIR, f\"{base}_face_{i}.jpg\")\n",
    "        cv2.imwrite(outp, crop)\n",
    "        count += 1\n",
    "print('Saved', count, 'anime face crops to', CROPS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build embedding model (TensorFlow MobileNetV2) and compute embeddings\n",
    "IMG_SIZE = 128\n",
    "base = tf.keras.applications.MobileNetV2(input_shape=(IMG_SIZE,IMG_SIZE,3), include_top=False, weights='imagenet')\n",
    "x = base.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(256, activation=None)(x)\n",
    "x = layers.Lambda(lambda t: tf.math.l2_normalize(t, axis=1))(x)\n",
    "embed_model = models.Model(inputs=base.input, outputs=x)\n",
    "\n",
    "crop_paths = sorted(glob.glob(os.path.join(CROPS_DIR, '*.jpg')))\n",
    "print('Found', len(crop_paths), 'crops')\n",
    "if len(crop_paths) > 0:\n",
    "    def load_img(path):\n",
    "        img = Image.open(path).convert('RGB').resize((IMG_SIZE,IMG_SIZE), Image.BICUBIC)\n",
    "        return np.asarray(img)/255.0\n",
    "    X = np.stack([load_img(p) for p in crop_paths], axis=0)\n",
    "    embs = embed_model.predict(X, batch_size=64)\n",
    "    np.save('data/embeddings.npy', embs)\n",
    "    with open('data/crop_paths.json', 'w') as f:\n",
    "        json.dump(crop_paths, f)\n",
    "    print('Saved embeddings and paths')\n",
    "else:\n",
    "    print('No crops found; run earlier cells to produce crops.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest-neighbor search with scikit-learn\n",
    "embs = np.load('data/embeddings.npy')\n",
    "with open('data/crop_paths.json','r') as f:\n",
    "    crop_paths = json.load(f)\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=50, metric='cosine')\n",
    "nn.fit(embs)\n",
    "\n",
    "# Multiple seeds\n",
    "def load_img(path, size=IMG_SIZE):\n",
    "    img = Image.open(path).convert('RGB').resize((size,size), Image.BICUBIC)\n",
    "    return np.asarray(img)/255.0\n",
    "\n",
    "# Provide a list of seed images (manually chosen Izutsumi crops)\n",
    "seed_paths = [\n",
    "    'data/crops/021_panel_4_crop_182.jpg',\n",
    "    'data/crops/001_panel_5_0_face_3.jpg',\n",
    "    'data/crops/012_panel_56_0_face_0.jpg',\n",
    "    'data/crops/011_panel_519_0_face_0.jpg',\n",
    "    'data/crops/011_panel_251_0_face_1.jpg',\n",
    "    'data/crops/020_panel_31_0_face_0.jpg',\n",
    "    'data/crops/021_panel_754_0_face_0.jpg',\n",
    "    'data/crops/021_panel_652_0_face_0.jpg',\n",
    "    'data/crops/022_panel_2244_face_0.jpg',\n",
    "    'data/crops/038_panel_420_0_face_0.jpg'\n",
    "]\n",
    "if len(seed_paths) > 0:\n",
    "    X_seed = np.stack([load_img(p) for p in seed_paths], axis=0)\n",
    "    seed_embs = embed_model.predict(X_seed, batch_size=8)\n",
    "    query_vec = np.mean(seed_embs, axis=0, keepdims=True)  # average embedding\n",
    "    query_vec = query_vec / np.linalg.norm(query_vec, axis=1, keepdims=True)\n",
    "\n",
    "    # Nearest neighbors\n",
    "    dists, idxs = nn.kneighbors(query_vec, n_neighbors=40)\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    for i, idx in enumerate(idxs[0]):\n",
    "        im = Image.open(crop_paths[idx]).convert('RGB')\n",
    "        plt.subplot(5, 8, i+1)\n",
    "        plt.imshow(im.resize((128,128)))\n",
    "        plt.title(f\"{dists[0,i]:.3f}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Add paths of Izutsumi crops to seed_paths list.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
